# Ollama 模型文件

> [!NOTE]
> `Modelfile` 语法仍在开发中

模型文件是创建和分享 Ollama 模型的蓝图。

## 目录

- [格式](#格式)
- [示例](#示例)
- [指令](#指令)
  - [FROM（必需）](#from-必需)
    - [从现有模型构建](#从现有模型构建)
    - [从 Safetensors 模型构建](#从-safetensors-模型构建)
    - [从 GGUF 文件构建](#从-gguf-文件构建)
  - [PARAMETER](#parameter)
    - [有效参数和值](#有效参数和值)
  - [TEMPLATE](#template)
    - [模板变量](#模板变量)
  - [SYSTEM](#system)
  - [ADAPTER](#adapter)
  - [LICENSE](#license)
  - [MESSAGE](#message)
- [注意事项](#注意事项)

## 格式

`Modelfile` 的格式：

```
# 注释
指令 参数
```

| 指令                              | 说明                              |
| --------------------------------- | --------------------------------- |
| [`FROM`](#from-必需)（必需）      | 定义要使用的基础模型。            |
| [`PARAMETER`](#parameter)         | 设置 Ollama 运行模型的参数。      |
| [`TEMPLATE`](#template)           | 发送到模型的完整提示模板。        |
| [`SYSTEM`](#system)               | 指定将在模板中设置的系统消息。    |
| [`ADAPTER`](#adapter)             | 定义要应用于模型的 (Q)LoRA 适配器。 |
| [`LICENSE`](#license)             | 指定法律许可证。                  |
| [`MESSAGE`](#message)             | 指定消息历史记录。                |

## 示例

### 基本 `Modelfile`

创建 mario 蓝图的 `Modelfile` 示例：

```
FROM llama3.2
# 将温度设置为 1 [更高意味着更有创意，更低意味着更连贯]
PARAMETER temperature 1
# 将上下文窗口大小设置为 4096，这控制了 LLM 可以用作生成下一个标记的上下文的标记数
PARAMETER num_ctx 4096

# 设置自定义系统消息以指定聊天助手的行为
SYSTEM You are Mario from super mario bros, acting as an assistant.
```

使用方法：

1. 将其保存为文件（例如 `Modelfile`）
2. `ollama create choose-a-model-name -f <文件位置，例如 ./Modelfile>`
3. `ollama run choose-a-model-name`
4. 开始使用模型！

要查看特定模型的 Modelfile，使用 `ollama show --modelfile` 命令。

```shell
ollama show --modelfile llama3.2
```

> **输出**：
>
> ```
> # Modelfile generated by "ollama show"
> # To build a new Modelfile based on this one, replace the FROM line with:
> # FROM llama3.2:latest
> FROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29
> TEMPLATE """{{ if .System }}<|start_header_id|>system<|end_header_id|>
>
> {{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>
>
> {{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>
>
> {{ .Response }}<|eot_id|>"""
> PARAMETER stop "<|start_header_id|>"
> PARAMETER stop "<|end_header_id|>"
> PARAMETER stop "<|eot_id|>"
> PARAMETER stop "<|reserved_special_token"
> ```


## 指令

### FROM（必需）

`FROM` 指令定义创建模型时使用的基础模型。

```
FROM <模型名称>:<标签>
```

#### 从现有模型构建

```
FROM llama3.2
```

可用的基础模型列表：
<https://github.com/ollama/ollama#model-library>
更多模型可在以下网址找到：
<https://ollama.com/library>

#### 从 Safetensors 模型构建

```
FROM <模型目录>
```

模型目录应包含支持架构的 Safetensors 权重。

目前支持的模型架构：
  * Llama（包括 Llama 2、Llama 3、Llama 3.1 和 Llama 3.2）
  * Mistral（包括 Mistral 1、Mistral 2 和 Mixtral）
  * Gemma（包括 Gemma 1 和 Gemma 2）
  * Phi3

#### 从 GGUF 文件构建

```
FROM ./ollama-model.gguf
```

GGUF 文件位置应指定为绝对路径或相对于 `Modelfile` 位置的路径。


### PARAMETER

`PARAMETER` 指令定义运行模型时可以设置的参数。

```
PARAMETER <参数> <参数值>
```

#### 有效参数和值

| 参数           | 说明                                                                                                                                                                     | 值类型 | 示例用法             |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | -------------------- |
| mirostat       | 启用 Mirostat 采样以控制困惑度。（默认值：0，0 = 禁用，1 = Mirostat，2 = Mirostat 2.0）                                                                                 | 整数   | mirostat 0           |
| mirostat_eta   | 影响算法对生成文本反馈的响应速度。较低的学习率将导致调整较慢，而较高的学习率将使算法反应更灵敏。（默认值：0.1）                                                         | 浮点数 | mirostat_eta 0.1     |
| mirostat_tau   | 控制输出的连贯性和多样性之间的平衡。较低的值将产生更集中和连贯的文本。（默认值：5.0）                                                                                   | 浮点数 | mirostat_tau 5.0     |
| num_ctx        | 设置用于生成下一个标记的上下文窗口大小。（默认值：2048）                                                                                                                | 整数   | num_ctx 4096         |
| repeat_last_n  | 设置模型回顾防止重复的距离。（默认值：64，0 = 禁用，-1 = num_ctx）                                                                                                       | 整数   | repeat_last_n 64     |
| repeat_penalty | 设置对重复内容的惩罚强度。较高的值（例如 1.5）将更强烈地惩罚重复，而较低的值（例如 0.9）将更宽容。（默认值：1.1）                                                        | 浮点数 | repeat_penalty 1.1   |
| temperature    | 模型的温度。增加温度将使模型回答更有创意。（默认值：0.8）                                                                                                               | 浮点数 | temperature 0.7      |
| seed           | 设置生成时使用的随机数种子。将其设置为特定数字将使模型为相同的提示生成相同的文本。（默认值：0）                                                                         | 整数   | seed 42              |
| stop           | 设置要使用的停止序列。当遇到此模式时，LLM 将停止生成文本并返回。可以通过在 modelfile 中指定多个单独的 `stop` 参数来设置多个停止模式。                                 | 字符串 | stop "AI assistant:" |
| num_predict    | 生成文本时预测的最大标记数。（默认值：-1，无限生成）                                                                                                                     | 整数   | num_predict 42       |
| top_k          | 降低生成无意义内容的概率。较高的值（例如 100）将给出更多样化的答案，而较低的值（例如 10）将更加保守。（默认值：40）                                                     | 整数   | top_k 40             |
| top_p          | 与 top-k 一起工作。较高的值（例如 0.95）将产生更多样化的文本，而较低的值（例如 0.5）将生成更集中和保守的文本。（默认值：0.9）                                           | 浮点数 | top_p 0.9            |
| min_p          | top_p 的替代方案，旨在确保质量和多样性的平衡。参数 *p* 表示相对于最可能标记的概率，一个标记被考虑的最小概率。例如，当 *p*=0.05 且最可能的标记概率为 0.9 时，小于 0.045 的 logits 将被过滤掉。（默认值：0.0） | 浮点数 | min_p 0.05            |

### TEMPLATE

要传递到模型中的完整提示模板的 `TEMPLATE`。它可以包括（可选）系统消息、用户消息和模型的响应。注意：语法可能因模型而异。模板使用 Go [模板语法](https://pkg.go.dev/text/template)。

#### 模板变量

| 变量              | 说明                                                           |
| ----------------- | -------------------------------------------------------------- |
| `{{ .System }}`   | 用于指定自定义行为的系统消息。                                 |
| `{{ .Prompt }}`   | 用户提示消息。                                                 |
| `{{ .Response }}` | 模型的响应。在生成响应时，此变量后的文本将被省略。             |

```
TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""
```

### SYSTEM

`SYSTEM` 指令指定在模板中使用的系统消息（如适用）。

```
SYSTEM """<系统消息>"""
```

### ADAPTER

`ADAPTER` 指令指定应用于基础模型的微调 LoRA 适配器。适配器的值应为绝对路径或相对于 Modelfile 的路径。基础模型应通过 `FROM` 指令指定。如果基础模型与适配器微调的基础模型不同，行为将不稳定。

#### Safetensor 适配器

```
ADAPTER <safetensor 适配器路径>
```

目前支持的 Safetensor 适配器：
  * Llama（包括 Llama 2、Llama 3 和 Llama 3.1）
  * Mistral（包括 Mistral 1、Mistral 2 和 Mixtral）
  * Gemma（包括 Gemma 1 和 Gemma 2）

#### GGUF 适配器

```
ADAPTER ./ollama-lora.gguf
```

### LICENSE

`LICENSE` 指令允许指定使用此 Modelfile 的模型共享或分发时的法律许可证。

```
LICENSE """
<许可证文本>
"""
```

### MESSAGE

`MESSAGE` 指令允许指定模型在回应时使用的消息历史记录。使用多个 MESSAGE 命令迭代来构建会话，从而指导模型以类似方式回答。

```
MESSAGE <角色> <消息>
```

#### 有效角色

| 角色      | 说明                                         |
| --------- | -------------------------------------------- |
| system    | 为模型提供 SYSTEM 消息的替代方式。           |
| user      | 用户可能提出的示例问题。                     |
| assistant | 模型应如何回应的示例消息。                   |


#### 会话示例

```
MESSAGE user 多伦多在加拿大吗？
MESSAGE assistant 是的
MESSAGE user 萨克拉门托在加拿大吗？
MESSAGE assistant 不是
MESSAGE user 安大略省在加拿大吗？
MESSAGE assistant 是的
```


## 注意事项

- **`Modelfile` 不区分大小写**。在示例中，使用大写指令是为了更容易区分它与参数。
- 指令可以按任意顺序排列。在示例中，`FROM` 指令放在首位以保持可读性。

[1]: https://ollama.com/library
